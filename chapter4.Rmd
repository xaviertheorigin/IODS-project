# 4. Clustering and classification
```{r, echo= FALSE, results='hide', message=FALSE}
library(MASS)
library(corrplot)
library(tidyverse)
```

## First look at the data
```{r}
data("Boston")
str(Boston)
dim(Boston)
```
The  *Boston* dataset contains 506 rows and 14 columns of information relevant to the housing values in the suburbs of Boston.  
  
## Graphical analysis  

```{r}
#We show a summary of the dataset
summary(Boston)
#We create a correlation matrix and store it
cor_matrix<-cor(Boston)
cor_matrix %>% round(digits = 2)
#We create a correlation plot
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
#We create a distribution plot
Boston %>% gather() %>% ggplot(aes(value)) + facet_wrap(~ key, scales = "free") + geom_density(colour="red")
```
  
We can easily observe thanks to the correlation plot the different interactions between the elements of the dataset. We also see in distribution graphics how the different variables do not represent a normal distribution.  

## Standarization and creation of the categorical variable
```{r}
#We center and standarized the variables
boston_scaled <- scale(Boston)
#We show a summary of the scaled variables
summary(boston_scaled)
# We show the class of the boston_scaled object
class(boston_scaled)
#We change the object to data frame to work later with it
boston_scaled <- as.data.frame(boston_scaled)
bins <- quantile(boston_scaled$crim)
#We create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate).
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low","med_low","med_high","high"))
#We remove the old crime rate variable from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
#We divide the dataset to train and test sets, so that 80% of the data belongs to the train set. 
n <- nrow(boston_scaled)
#We choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
#We create train set
train <- boston_scaled[ind,]
#We create test set 
test <- boston_scaled[-ind,]
```
  
## Fitting the linear discriminant analsysis on the train set  
```{r}
#We use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables
lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# We draw the plot with the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```
  
## Prediction of the classes   
```{r}
#We save the correct classes from test data
correct_classes <- test$crime
#We remove the crime variable from test data
test <- dplyr::select(test, -crime)
#We predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
#We cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```
As we can observe from the cross tabulation, our model predicts best the category of *high*. Second, the category of *low* is also high enough. Finally, the *med_low* and *med_high* categories have lower accuracy of predictions, even though the majority is still correct.  
  
  
## Distances and k-means  
```{r}
#We load the data and standarize it
data(Boston)
boston_scaled <- scale(Boston)
```
```{r results='hide', message=FALSE}
#We calculate the distances with the Eucidean distance
dist(boston_scaled)
```
```{r}
#We run k-means algorithm on the dataset.
km <-kmeans(boston_scaled, centers = 2)
#We visualize the clusters
pairs(boston_scaled, col = km$cluster)
```
  
From the options we can choose, 2 centers seem to be the best representation of the results, as we can see clearly differentiated clusters with such number. With more centers, the clusters are not separated properly and they tend to occupy the same space of our plots.  

## Super-bonus
```{r}
library(plotly)
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = classes)
```